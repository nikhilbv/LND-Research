<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>research.html</title>
  <meta name="generator" content="Haroopad 0.13.1" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <style>div.oembedall-githubrepos{border:1px solid #DDD;border-radius:4px;list-style-type:none;margin:0 0 10px;padding:8px 10px 0;font:13.34px/1.4 helvetica,arial,freesans,clean,sans-serif;width:452px;background-color:#fff}div.oembedall-githubrepos .oembedall-body{background:-moz-linear-gradient(center top,#FAFAFA,#EFEFEF);background:-webkit-gradient(linear,left top,left bottom,from(#FAFAFA),to(#EFEFEF));border-bottom-left-radius:4px;border-bottom-right-radius:4px;border-top:1px solid #EEE;margin-left:-10px;margin-top:8px;padding:5px 10px;width:100%}div.oembedall-githubrepos h3{font-size:14px;margin:0;padding-left:18px;white-space:nowrap}div.oembedall-githubrepos p.oembedall-description{color:#444;font-size:12px;margin:0 0 3px}div.oembedall-githubrepos p.oembedall-updated-at{color:#888;font-size:11px;margin:0}div.oembedall-githubrepos ul.oembedall-repo-stats{border:none;float:right;font-size:11px;font-weight:700;padding-left:15px;position:relative;z-index:5;margin:0}div.oembedall-githubrepos ul.oembedall-repo-stats li{border:none;color:#666;display:inline-block;list-style-type:none;margin:0!important}div.oembedall-githubrepos ul.oembedall-repo-stats li a{background-color:transparent;border:none;color:#666!important;background-position:5px -2px;background-repeat:no-repeat;border-left:1px solid #DDD;display:inline-block;height:21px;line-height:21px;padding:0 5px 0 23px}div.oembedall-githubrepos ul.oembedall-repo-stats li:first-child a{border-left:medium none;margin-right:-3px}div.oembedall-githubrepos ul.oembedall-repo-stats li a:hover{background:5px -27px no-repeat #4183C4;color:#FFF!important;text-decoration:none}div.oembedall-githubrepos ul.oembedall-repo-stats li:first-child a:hover{border-bottom-left-radius:3px;border-top-left-radius:3px}ul.oembedall-repo-stats li:last-child a:hover{border-bottom-right-radius:3px;border-top-right-radius:3px}span.oembedall-closehide{background-color:#aaa;border-radius:2px;cursor:pointer;margin-right:3px}div.oembedall-container{margin-top:5px;text-align:left}.oembedall-ljuser{font-weight:700}.oembedall-ljuser img{vertical-align:bottom;border:0;padding-right:1px}.oembedall-stoqembed{border-bottom:1px dotted #999;float:left;overflow:hidden;width:730px;line-height:1;background:#FFF;color:#000;font-family:Arial,Liberation Sans,DejaVu Sans,sans-serif;font-size:80%;text-align:left;margin:0;padding:0}.oembedall-stoqembed a{color:#07C;text-decoration:none;margin:0;padding:0}.oembedall-stoqembed a:hover{text-decoration:underline}.oembedall-stoqembed a:visited{color:#4A6B82}.oembedall-stoqembed h3{font-family:Trebuchet MS,Liberation Sans,DejaVu Sans,sans-serif;font-size:130%;font-weight:700;margin:0;padding:0}.oembedall-stoqembed .oembedall-reputation-score{color:#444;font-size:120%;font-weight:700;margin-right:2px}.oembedall-stoqembed .oembedall-user-info{height:35px;width:185px}.oembedall-stoqembed .oembedall-user-info .oembedall-user-gravatar32{float:left;height:32px;width:32px}.oembedall-stoqembed .oembedall-user-info .oembedall-user-details{float:left;margin-left:5px;overflow:hidden;white-space:nowrap;width:145px}.oembedall-stoqembed .oembedall-question-hyperlink{font-weight:700}.oembedall-stoqembed .oembedall-stats{background:#EEE;margin:0 0 0 7px;padding:4px 7px 6px;width:58px}.oembedall-stoqembed .oembedall-statscontainer{float:left;margin-right:8px;width:86px}.oembedall-stoqembed .oembedall-votes{color:#555;padding:0 0 7px;text-align:center}.oembedall-stoqembed .oembedall-vote-count-post{font-size:240%;color:#808185;display:block;font-weight:700}.oembedall-stoqembed .oembedall-views{color:#999;padding-top:4px;text-align:center}.oembedall-stoqembed .oembedall-status{margin-top:-3px;padding:4px 0;text-align:center;background:#75845C;color:#FFF}.oembedall-stoqembed .oembedall-status strong{color:#FFF;display:block;font-size:140%}.oembedall-stoqembed .oembedall-summary{float:left;width:635px}.oembedall-stoqembed .oembedall-excerpt{line-height:1.2;margin:0;padding:0 0 5px}.oembedall-stoqembed .oembedall-tags{float:left;line-height:18px}.oembedall-stoqembed .oembedall-tags a:hover{text-decoration:none}.oembedall-stoqembed .oembedall-post-tag{background-color:#E0EAF1;border-bottom:1px solid #3E6D8E;border-right:1px solid #7F9FB6;color:#3E6D8E;font-size:90%;line-height:2.4;margin:2px 2px 2px 0;padding:3px 4px;text-decoration:none;white-space:nowrap}.oembedall-stoqembed .oembedall-post-tag:hover{background-color:#3E6D8E;border-bottom:1px solid #37607D;border-right:1px solid #37607D;color:#E0EAF1}.oembedall-stoqembed .oembedall-fr{float:right}.oembedall-stoqembed .oembedall-statsarrow{background-image:url(http://cdn.sstatic.net/stackoverflow/img/sprites.png?v=3);background-repeat:no-repeat;overflow:hidden;background-position:0 -435px;float:right;height:13px;margin-top:12px;width:7px}.oembedall-facebook1{border:1px solid #1A3C6C;padding:0;font:13.34px/1.4 verdana;width:500px}.oembedall-facebook2{background-color:#627add}.oembedall-facebook2 a{color:#e8e8e8;text-decoration:none}.oembedall-facebookBody{background-color:#fff;vertical-align:top;padding:5px}.oembedall-facebookBody .contents{display:inline-block;width:100%}.oembedall-facebookBody div img{float:left;margin-right:5px}div.oembedall-lanyard{-webkit-box-shadow:none;-webkit-transition-delay:0s;-webkit-transition-duration:.4000000059604645s;-webkit-transition-property:width;-webkit-transition-timing-function:cubic-bezier(0.42,0,.58,1);background-attachment:scroll;background-clip:border-box;background-color:transparent;background-image:none;background-origin:padding-box;border-width:0;box-shadow:none;color:#112644;display:block;float:left;font-family:'Trebuchet MS',Trebuchet,sans-serif;font-size:16px;height:253px;line-height:19px;margin:0;max-width:none;min-height:0;outline:#112644 0;overflow-x:visible;overflow-y:visible;padding:0;position:relative;text-align:left;vertical-align:baseline;width:804px}div.oembedall-lanyard .tagline{font-size:1.5em}div.oembedall-lanyard .wrapper{overflow:hidden;clear:both}div.oembedall-lanyard .split{float:left;display:inline}div.oembedall-lanyard .prominent-place .flag:active,div.oembedall-lanyard .prominent-place .flag:focus,div.oembedall-lanyard .prominent-place .flag:hover,div.oembedall-lanyard .prominent-place .flag:link,div.oembedall-lanyard .prominent-place .flag:visited{float:left;display:block;width:48px;height:48px;position:relative;top:-5px;margin-right:10px}div.oembedall-lanyard .place-context{font-size:.889em}div.oembedall-lanyard .prominent-place .sub-place{display:block}div.oembedall-lanyard .prominent-place{font-size:1.125em;line-height:1.1em;font-weight:400}div.oembedall-lanyard .main-date{color:#8CB4E0;font-weight:700;line-height:1.1}div.oembedall-lanyard .first{width:48.57%;margin:0 0 0 2.857%}.mermaid .label{color:#333}.node circle,.node polygon,.node rect{fill:#cde498;stroke:#13540c;stroke-width:1px}.edgePath .path{stroke:green;stroke-width:1.5px}.cluster rect{fill:#cdffb2;rx:40;stroke:#6eaa49;stroke-width:1px}.cluster text{fill:#333}.actor{stroke:#13540c;fill:#cde498}text.actor{fill:#000;stroke:none}.actor-line{stroke:grey}.messageLine0{stroke-width:1.5;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#333}.messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#arrowhead{fill:#333}#crosshead path{fill:#333!important;stroke:#333!important}.messageText{fill:#333;stroke:none}.labelBox{stroke:#326932;fill:#cde498}.labelText,.loopText{fill:#000;stroke:none}.loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#326932}.note{stroke:#6eaa49;fill:#fff5ad}.noteText{fill:#000;stroke:none;font-family:'trebuchet ms',verdana,arial;font-size:14px}.section{stroke:none;opacity:.2}.section0,.section2{fill:#6eaa49}.section1,.section3{fill:#fff;opacity:.2}.sectionTitle0,.sectionTitle1,.sectionTitle2,.sectionTitle3{fill:#333}.sectionTitle{text-anchor:start;font-size:11px;text-height:14px}.grid .tick{stroke:lightgrey;opacity:.3;shape-rendering:crispEdges}.grid path{stroke-width:0}.today{fill:none;stroke:red;stroke-width:2px}.task{stroke-width:2}.taskText{text-anchor:middle;font-size:11px}.taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}.taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}.taskText0,.taskText1,.taskText2,.taskText3{fill:#fff}.task0,.task1,.task2,.task3{fill:#487e3a;stroke:#13540c}.taskTextOutside0,.taskTextOutside1,.taskTextOutside2,.taskTextOutside3{fill:#000}.active0,.active1,.active2,.active3{fill:#cde498;stroke:#13540c}.activeText0,.activeText1,.activeText2,.activeText3{fill:#000!important}.done0,.done1,.done2,.done3{stroke:grey;fill:lightgrey;stroke-width:2}.doneText0,.doneText1,.doneText2,.doneText3{fill:#000!important}.crit0,.crit1,.crit2,.crit3{stroke:#f88;fill:red;stroke-width:2}.activeCrit0,.activeCrit1,.activeCrit2,.activeCrit3{stroke:#f88;fill:#cde498;stroke-width:2}.doneCrit0,.doneCrit1,.doneCrit2,.doneCrit3{stroke:#f88;fill:lightgrey;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}.activeCritText0,.activeCritText1,.activeCritText2,.activeCritText3,.doneCritText0,.doneCritText1,.doneCritText2,.doneCritText3{fill:#000!important}.titleText{text-anchor:middle;font-size:18px;fill:#000}text{font-family:'trebuchet ms',verdana,arial;font-size:14px}html{height:100%}body{margin:0!important;padding:5px 20px 26px!important;background-color:#fff;font-family:"Lucida Grande","Segoe UI","Apple SD Gothic Neo","Malgun Gothic","Lucida Sans Unicode",Helvetica,Arial,sans-serif;font-size:.9em;overflow-x:hidden;overflow-y:auto}br,h1,h2,h3,h4,h5,h6{clear:both}hr.page{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x;border:0;height:3px;padding:0}hr.underscore{border-top-style:dashed!important}body >:first-child{margin-top:0!important}img.plugin{box-shadow:0 1px 3px rgba(0,0,0,.1);border-radius:3px}iframe{border:0}figure{-webkit-margin-before:0;-webkit-margin-after:0;-webkit-margin-start:0;-webkit-margin-end:0}kbd{border:1px solid #aaa;-moz-border-radius:2px;-webkit-border-radius:2px;border-radius:2px;-moz-box-shadow:1px 2px 2px #ddd;-webkit-box-shadow:1px 2px 2px #ddd;box-shadow:1px 2px 2px #ddd;background-color:#f9f9f9;background-image:-moz-linear-gradient(top,#eee,#f9f9f9,#eee);background-image:-o-linear-gradient(top,#eee,#f9f9f9,#eee);background-image:-webkit-linear-gradient(top,#eee,#f9f9f9,#eee);background-image:linear-gradient(top,#eee,#f9f9f9,#eee);padding:1px 3px;font-family:inherit;font-size:.85em}.oembeded .oembed_photo{display:inline-block}img[data-echo]{margin:25px 0;width:100px;height:100px;background:url(../img/ajax.gif) center center no-repeat #fff}.spinner{display:inline-block;width:10px;height:10px;margin-bottom:-.1em;border:2px solid rgba(0,0,0,.5);border-top-color:transparent;border-radius:100%;-webkit-animation:spin 1s infinite linear;animation:spin 1s infinite linear}.spinner:after{content:'';display:block;width:0;height:0;position:absolute;top:-6px;left:0;border:4px solid transparent;border-bottom-color:rgba(0,0,0,.5);-webkit-transform:rotate(45deg);transform:rotate(45deg)}@-webkit-keyframes spin{to{-webkit-transform:rotate(360deg)}}@keyframes spin{to{transform:rotate(360deg)}}p.toc{margin:0!important}p.toc ul{padding-left:10px}p.toc>ul{padding:10px;margin:0 10px;display:inline-block;border:1px solid #ededed;border-radius:5px}p.toc li,p.toc ul{list-style-type:none}p.toc li{width:100%;padding:0;overflow:hidden}p.toc li a::after{content:"."}p.toc li a:before{content:"• "}p.toc h5{text-transform:uppercase}p.toc .title{float:left;padding-right:3px}p.toc .number{margin:0;float:right;padding-left:3px;background:#fff;display:none}input.task-list-item{margin-left:-1.62em}.markdown{font-family:"Hiragino Sans GB","Microsoft YaHei",STHeiti,SimSun,"Lucida Grande","Lucida Sans Unicode","Lucida Sans",'Segoe UI',AppleSDGothicNeo-Medium,'Malgun Gothic',Verdana,Tahoma,sans-serif;padding:20px}.markdown a{text-decoration:none;vertical-align:baseline}.markdown a:hover{text-decoration:underline}.markdown h1{font-size:2.2em;font-weight:700;margin:1.5em 0 1em}.markdown h2{font-size:1.8em;font-weight:700;margin:1.275em 0 .85em}.markdown h3{font-size:1.6em;font-weight:700;margin:1.125em 0 .75em}.markdown h4{font-size:1.4em;font-weight:700;margin:.99em 0 .66em}.markdown h5{font-size:1.2em;font-weight:700;margin:.855em 0 .57em}.markdown h6{font-size:1em;font-weight:700;margin:.75em 0 .5em}.markdown h1+p,.markdown h1:first-child,.markdown h2+p,.markdown h2:first-child,.markdown h3+p,.markdown h3:first-child,.markdown h4+p,.markdown h4:first-child,.markdown h5+p,.markdown h5:first-child,.markdown h6+p,.markdown h6:first-child{margin-top:0}.markdown hr{border:1px solid #ccc}.markdown p{margin:1em 0;word-wrap:break-word}.markdown ol{list-style-type:decimal}.markdown li{display:list-item;line-height:1.4em}.markdown blockquote{margin:1em 20px}.markdown blockquote>:first-child{margin-top:0}.markdown blockquote>:last-child{margin-bottom:0}.markdown blockquote cite:before{content:'\2014 \00A0'}.markdown .code{border-radius:3px;word-wrap:break-word}.markdown pre{border-radius:3px;word-wrap:break-word;border:1px solid #ccc;overflow:auto;padding:.5em}.markdown pre code{border:0;display:block}.markdown pre>code{font-family:Consolas,Inconsolata,Courier,monospace;font-weight:700;white-space:pre;margin:0}.markdown code{border-radius:3px;word-wrap:break-word;border:1px solid #ccc;padding:0 5px;margin:0 2px}.markdown img{max-width:100%}.markdown mark{color:#000;background-color:#fcf8e3}.markdown table{padding:0;border-collapse:collapse;border-spacing:0;margin-bottom:16px}.markdown table tr td,.markdown table tr th{border:1px solid #ccc;margin:0;padding:6px 13px}.markdown table tr th{font-weight:700}.markdown table tr th>:first-child{margin-top:0}.markdown table tr th>:last-child{margin-bottom:0}.markdown table tr td>:first-child{margin-top:0}.markdown table tr td>:last-child{margin-bottom:0}.github{padding:20px;font-family:"Helvetica Neue",Helvetica,"Hiragino Sans GB","Microsoft YaHei",STHeiti,SimSun,"Segoe UI",AppleSDGothicNeo-Medium,'Malgun Gothic',Arial,freesans,sans-serif;font-size:15px;background:#fff;line-height:1.6;-webkit-font-smoothing:antialiased}.github a{color:#3269a0}.github a:hover{color:#4183c4}.github h2{border-bottom:1px solid #e6e6e6;line-height:1.6}.github h6{color:#777}.github hr{border:1px solid #e6e6e6}.github pre>code{font-size:.9em;font-family:Consolas,Inconsolata,Courier,monospace}.github blockquote>code,.github h1>code,.github h2>code,.github h3>code,.github h4>code,.github h5>code,.github h6>code,.github li>code,.github p>code,.github td>code{background-color:rgba(0,0,0,.07);font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;font-size:85%;padding:.2em .5em;border:0}.github blockquote{border-left:4px solid #e6e6e6;padding:0 15px;font-style:italic}.github table{background-color:#fafafa}.github table tr td,.github table tr th{border:1px solid #e6e6e6}.github table tr:nth-child(2n){background-color:#f2f2f2}.hljs{display:block;overflow-x:auto;padding:.5em;background:#fdf6e3;color:#657b83;-webkit-text-size-adjust:none}.diff .hljs-header,.hljs-comment,.hljs-doctype,.hljs-javadoc,.hljs-pi,.lisp .hljs-string{color:#93a1a1}.css .hljs-tag,.hljs-addition,.hljs-keyword,.hljs-request,.hljs-status,.hljs-winutils,.method,.nginx .hljs-title{color:#859900}.hljs-command,.hljs-dartdoc,.hljs-hexcolor,.hljs-link_url,.hljs-number,.hljs-phpdoc,.hljs-regexp,.hljs-rules .hljs-value,.hljs-string,.hljs-tag .hljs-value,.tex .hljs-formula{color:#2aa198}.css .hljs-function,.hljs-built_in,.hljs-chunk,.hljs-decorator,.hljs-id,.hljs-identifier,.hljs-localvars,.hljs-title,.vhdl .hljs-literal{color:#268bd2}.hljs-attribute,.hljs-class .hljs-title,.hljs-constant,.hljs-link_reference,.hljs-parent,.hljs-type,.hljs-variable,.lisp .hljs-body,.smalltalk .hljs-number{color:#b58900}.css .hljs-pseudo,.diff .hljs-change,.hljs-attr_selector,.hljs-cdata,.hljs-header,.hljs-pragma,.hljs-preprocessor,.hljs-preprocessor .hljs-keyword,.hljs-shebang,.hljs-special,.hljs-subst,.hljs-symbol,.hljs-symbol .hljs-string{color:#cb4b16}.hljs-deletion,.hljs-important{color:#dc322f}.hljs-link_label{color:#6c71c4}.tex .hljs-formula{background:#eee8d5}.MathJax_Hover_Frame{border-radius:.25em;-webkit-border-radius:.25em;-moz-border-radius:.25em;-khtml-border-radius:.25em;box-shadow:0 0 15px #83A;-webkit-box-shadow:0 0 15px #83A;-moz-box-shadow:0 0 15px #83A;-khtml-box-shadow:0 0 15px #83A;border:1px solid #A6D!important;display:inline-block;position:absolute}.MathJax_Hover_Arrow{position:absolute;width:15px;height:11px;cursor:pointer}#MathJax_About{position:fixed;left:50%;width:auto;text-align:center;border:3px outset;padding:1em 2em;background-color:#DDD;color:#000;cursor:default;font-family:message-box;font-size:120%;font-style:normal;text-indent:0;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;z-index:201;border-radius:15px;-webkit-border-radius:15px;-moz-border-radius:15px;-khtml-border-radius:15px;box-shadow:0 10px 20px gray;-webkit-box-shadow:0 10px 20px gray;-moz-box-shadow:0 10px 20px gray;-khtml-box-shadow:0 10px 20px gray;filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}.MathJax_Menu{position:absolute;background-color:#fff;color:#000;width:auto;padding:5px 0;border:1px solid #CCC;margin:0;cursor:default;font:menu;text-align:left;text-indent:0;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;z-index:201;border-radius:5px;-webkit-border-radius:5px;-moz-border-radius:5px;-khtml-border-radius:5px;box-shadow:0 10px 20px gray;-webkit-box-shadow:0 10px 20px gray;-moz-box-shadow:0 10px 20px gray;-khtml-box-shadow:0 10px 20px gray;filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}.MathJax_MenuItem{padding:1px 2em;background:0 0}.MathJax_MenuArrow{position:absolute;right:.5em;color:#666}.MathJax_MenuActive .MathJax_MenuArrow{color:#fff}.MathJax_MenuArrow.RTL{left:.5em;right:auto}.MathJax_MenuCheck{position:absolute;left:.7em}.MathJax_MenuCheck.RTL{right:.7em;left:auto}.MathJax_MenuRadioCheck{position:absolute;left:.7em}.MathJax_MenuRadioCheck.RTL{right:.7em;left:auto}.MathJax_MenuLabel{padding:1px 2em 3px 1.33em;font-style:italic}.MathJax_MenuRule{border-top:1px solid #DDD;margin:4px 3px}.MathJax_MenuDisabled{color:GrayText}.MathJax_MenuActive{background-color:#606872;color:#fff}.MathJax_Menu_Close{position:absolute;width:31px;height:31px;top:-15px;left:-15px}#MathJax_Zoom{position:absolute;background-color:#F0F0F0;overflow:auto;display:block;z-index:301;padding:.5em;border:1px solid #000;margin:0;font-weight:400;font-style:normal;text-align:left;text-indent:0;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;box-shadow:5px 5px 15px #AAA;-webkit-box-shadow:5px 5px 15px #AAA;-moz-box-shadow:5px 5px 15px #AAA;-khtml-box-shadow:5px 5px 15px #AAA;filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}#MathJax_ZoomOverlay{position:absolute;left:0;top:0;z-index:300;display:inline-block;width:100%;height:100%;border:0;padding:0;margin:0;background-color:#fff;opacity:0;filter:alpha(opacity=0)}#MathJax_ZoomFrame{position:relative;display:inline-block;height:0;width:0}#MathJax_ZoomEventTrap{position:absolute;left:0;top:0;z-index:302;display:inline-block;border:0;padding:0;margin:0;background-color:#fff;opacity:0;filter:alpha(opacity=0)}.MathJax_Preview{color:#888}#MathJax_Message{position:fixed;left:1px;bottom:2px;background-color:#E6E6E6;border:1px solid #959595;margin:0;padding:2px 8px;z-index:102;color:#000;font-size:80%;width:auto;white-space:nowrap}#MathJax_MSIE_Frame{position:absolute;top:0;left:0;width:0;z-index:101;border:0;margin:0;padding:0}.MathJax_Error{color:#C00;font-style:italic}footer{position:fixed;font-size:.8em;text-align:right;bottom:0;margin-left:-25px;height:20px;width:100%}</style>
</head>
<body class="markdown github">
<h2 id="problems-apollo-baidu-has-solved-using-ai"><a name="problems-apollo-baidu-has-solved-using-ai" href="#problems-apollo-baidu-has-solved-using-ai"></a>Problems Apollo Baidu has solved using AI</h2><h4 id="1.-lidar-point-cloud-obstacle-detection-&amp;-classification"><a name="1.-lidar-point-cloud-obstacle-detection-&amp;-classification" href="#1.-lidar-point-cloud-obstacle-detection-&amp;-classification"></a>1. Lidar Point Cloud Obstacle Detection &amp; Classification</h4><ul>
<li>Input - Lidar data</li><li>Annotation type - 3D bbox</li><li>Labels - pedestrians, vehicles, non-motor vehicles (cyclists) and others (dontCares)</li><li>The Laser scanner used for the data acquisition is Velodyne HDL-64E S3</li><li>More details - <a href="http://data.apollo.auto/help?name=data_intro_3d&amp;data_key=lidar_obstacle_label&amp;data_type=0&amp;locale=en-us&amp;lang=en">http://data.apollo.auto/help?name=data_intro_3d&amp;data_key=lidar_obstacle_label&amp;data_type=0&amp;locale=en-us&amp;lang=en</a></li></ul><h4 id="2.-traffic-lights-detection"><a name="2.-traffic-lights-detection" href="#2.-traffic-lights-detection"></a>2. Traffic Lights Detection</h4><ul>
<li>Input - 1080P colorized images</li><li>Annotation type - 2D bbox</li><li>Annotations include: locations of traffic lights, colors of lights (red, yellow, green, black), shapes of lights (round, arrow, etc.), types of lights (horizontal, vertical, square, etc.) and other information</li><li>Two cameras for the data collection of traffic lights are mounted on the car roof, one of which uses a lens with a focal length of 25mm and the other uses a lens with a focal length of 6mm </li><li>More details - <a href="http://data.apollo.auto/help?name=data_intro_2d&amp;data_key=traffic_light_label&amp;data_type=0&amp;locale=en-us&amp;lang=en">http://data.apollo.auto/help?name=data_intro_2d&amp;data_key=traffic_light_label&amp;data_type=0&amp;locale=en-us&amp;lang=en</a></li></ul><h4 id="3.-road-hackers"><a name="3.-road-hackers" href="#3.-road-hackers"></a>3. Road Hackers</h4><ul>
<li>Dataset provides two types of data including images in front of vehicles and the vehicle motion status</li><li>The vehicle motion status data include the current speed and the track curvature</li><li>The data of Road Hackers come from the original data of the sensor, including images, laser radars, radars, etc., which are mainly input in the form of images</li><li>It outputs the vehicle’s control instructions, such as the steering wheel angle, acceleration, and braking</li><li>The input and output are connected through the deep neural network, that is, to directly generate the vehicle control instructions through the neural network to carry out the horizontal control and vertical control of the vehicle</li><li>Horizontal control : steering wheel angle</li><li>Vertical control : acceleration and braking</li><li>The platform mainly uses the horizontal control model. It trains the steering wheel control model through the images in front of the vehicle collected by the map collection cars</li><li>The output here does not use the steering wheel angle. It uses the curvature to be driven (ie, the reciprocal of the turning radius). The reasons are as follows:<br>1 The curvature is more universally applicable, which is not affected by the vehicle’s own parameters such as steering ratio, wheel base and so on.<br>2 The relationship between the curvature and the steering wheel angle is simple, which can be retrieved through the Ackermann model at a low speed and be fitted through a simple network at a high speed.</li><li>More details - <a href="http://data.apollo.auto/help?name=data_intro_roadhacker&amp;data_key=road_hackers&amp;data_type=0&amp;locale=en-us&amp;lang=en">http://data.apollo.auto/help?name=data_intro_roadhacker&amp;data_key=road_hackers&amp;data_type=0&amp;locale=en-us&amp;lang=en</a></li></ul><h4 id="4.-obstacle-detection[-image-based-]"><a name="4.-obstacle-detection[-image-based-]" href="#4.-obstacle-detection[-image-based-]"></a>4. Obstacle Detection[ Image based ]</h4><ul>
<li>Input - 1080P colorized images</li><li>Annotation type - 2D bbox</li><li>Labels - vehicles, pedestrians, cyclists, tricycle and other unmovable obstacles on the road</li><li>Two cameras for the data collection of obstacles are mounted on the car roof, one of which uses a lens with a focal length of 6mm and the other uses a lens with a focal length of 12mm</li><li>More details - <a href="http://data.apollo.auto/help?name=data-2d-obstacle-intro&amp;data_key=2d_obstacle_label&amp;data_type=0&amp;locale=en-us&amp;lang=en">http://data.apollo.auto/help?name=data-2d-obstacle-intro&amp;data_key=2d_obstacle_label&amp;data_type=0&amp;locale=en-us&amp;lang=en</a></li></ul><h4 id="5.-obstacle-trajectory-prediction"><a name="5.-obstacle-trajectory-prediction" href="#5.-obstacle-trajectory-prediction"></a>5. Obstacle Trajectory Prediction</h4><ul>
<li>The dataset is sampled from the real road scene, and consists of the real motor vehicle obstacles on the road</li><li>It contains the historical data of the vehicle multi-frame information, and obstacle features and lane features generated by a series of perception processing</li><li>Labelling is as follows<table>
<thead>
<tr>
<th>Label</th>
<th>Introduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>-1</td>
<td>The vehicle is not within the specific lane and will not cut in there in 1 second</td>
</tr>
<tr>
<td>0</td>
<td>The vehicle is within the specific lane but it will not keep this lane in 1 second</td>
</tr>
<tr>
<td>1</td>
<td>The vehicle is within the specific lane and it will keep this lane in 1 second</td>
</tr>
<tr>
<td>2</td>
<td>The vehicle is not within the specific lane but will cut in there in 1 second</td>
</tr>
</tbody>
</table>
</li></ul><ul>
<li>Sensors Integration: It includes Lidar (Velodyne HDL-64E S3), two cameras (one uses a 25mm focal length lens and the other uses a 6mm focal length lens), high definition map and localization system</li></ul><ul>
<li>More details - <a href="http://data.apollo.auto/help?name=data_intro_pnc&amp;data_key=prediction_label&amp;data_type=0&amp;locale=en-us&amp;lang=en">http://data.apollo.auto/help?name=data_intro_pnc&amp;data_key=prediction_label&amp;data_type=0&amp;locale=en-us&amp;lang=en</a></li></ul><h4 id="6.-scene-parsing-dataset"><a name="6.-scene-parsing-dataset" href="#6.-scene-parsing-dataset"></a>6. Scene Parsing Dataset</h4><ul>
<li>The whole dataset will include RGB videos with ten thousands of high resolution images and per pixel annotation, survey-grade dense 3D points with semantic segmentation, stereoscopic video, and panoramic images</li><li>Sensors and Data Acquisition:<br>— mid-size SUV with RIEGL VMX-1HA mobile mapping system<br>— two LiDAR sensors (500 scan line per second, range 420m, field of view 360 degrees)<br>— INS/GNSS unit<br>— two front cameras (VMX-CS6, 3384 x 2710). (Images are captured every one meter)</li></ul><table>
<thead>
<tr>
<th>Group</th>
<th>Class</th>
<th>Label ID</th>
<th>Color (RGB)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sky</td>
<td>Sky</td>
<td>17</td>
<td>(70,130,180)</td>
<td>-</td>
</tr>
<tr>
<td>Movable object</td>
<td>Car</td>
<td>33</td>
<td>(0, 0,142)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Motorcycle</td>
<td>34</td>
<td>(0, 0,230)</td>
<td>Parked without rider</td>
</tr>
<tr>
<td>-</td>
<td>Bicycle</td>
<td>35</td>
<td>(119, 11, 32)</td>
<td>Parked without rider</td>
</tr>
<tr>
<td>-</td>
<td>Pedestrian</td>
<td>36</td>
<td>(220, 20, 60)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Rider</td>
<td>37</td>
<td>(255, 0, 0)</td>
<td>Including bicycle, motorcycle, etc.</td>
</tr>
<tr>
<td>-</td>
<td>Truck</td>
<td>38</td>
<td>(0, 0, 70)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Bus</td>
<td>39</td>
<td>(0, 60,100)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Pedicab</td>
<td>40</td>
<td>(0, 0, 90)</td>
<td>-</td>
</tr>
<tr>
<td>Flat</td>
<td>Road</td>
<td>49</td>
<td>(128, 64,128)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Sidewalk</td>
<td>50</td>
<td>(244, 35,232)</td>
<td>-</td>
</tr>
<tr>
<td>Barrier</td>
<td>Traffic Cone</td>
<td>65</td>
<td>(152,251,152)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Road Piles</td>
<td>66</td>
<td>(180,165,180)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Fence</td>
<td>67</td>
<td>(190,153,153)</td>
<td>Fence including any holes</td>
</tr>
<tr>
<td>Static Object</td>
<td>Traffic Light</td>
<td>81</td>
<td>(250,170, 30)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Pole</td>
<td>82</td>
<td>(153,153,153)</td>
<td>both horizontal and vertical</td>
</tr>
<tr>
<td>-</td>
<td>Traffic Sign</td>
<td>83</td>
<td>(220,220, 0)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Wall</td>
<td>84</td>
<td>(102,102,156)</td>
<td>The wall near the roadside</td>
</tr>
<tr>
<td>-</td>
<td>Trash Can</td>
<td>85</td>
<td>(0, 0,110)</td>
<td>Not included in evaluation</td>
</tr>
<tr>
<td>-</td>
<td>Billboard</td>
<td>86</td>
<td>(0, 80,100)</td>
<td>Not included in evaluation</td>
</tr>
<tr>
<td>Construction</td>
<td>Building</td>
<td>97</td>
<td>(70, 70, 70)</td>
<td>-</td>
</tr>
<tr>
<td>-</td>
<td>Bridge</td>
<td>98</td>
<td>(150,100,100)</td>
<td>Not included in evaluation</td>
</tr>
<tr>
<td>-</td>
<td>Tunnel</td>
<td>99</td>
<td>(150,120, 90)</td>
<td>Not included in evaluation</td>
</tr>
<tr>
<td>-</td>
<td>Overpass</td>
<td>100</td>
<td>(250,170,160)</td>
<td>-</td>
</tr>
<tr>
<td>Nature</td>
<td>Vegetation</td>
<td>113</td>
<td>(107,142, 35)</td>
<td>-</td>
</tr>
<tr>
<td>Others</td>
<td>Others</td>
<td>0</td>
<td>(0,0, 0)</td>
<td>Not included in evaluation</td>
</tr>
</tbody>
</table><h2 id="road-profile-dnn-hunt"><a name="road-profile-dnn-hunt" href="#road-profile-dnn-hunt"></a>Road Profile DNN Hunt</h2><h4 id="*-road-profiles--"><a name="*-road-profiles--" href="#*-road-profiles--"></a>* Road profiles -</h4><ul>
<li>road polygons</li><li>sidewalks</li><li><p>lane marking</p>
<ul>
<li>Broken White Line</li><li>Continuous White Line</li><li>Continuous Yellow Line</li><li>Double Continuous Yellow Line</li><li>Broken Yellow Line</li></ul>
</li><li><p>lanes</p>
<ul>
<li>Ego lanes and other road lanes</li></ul>
</li><li><p>road edges</p>
</li></ul><h4 id="*-road-profile-architectures--"><a name="*-road-profile-architectures--" href="#*-road-profile-architectures--"></a>* Road profile architectures -</h4><table>
<thead>
<tr>
<th>Sno</th>
<th>Application Area</th>
<th>Year</th>
<th>DNNArch</th>
<th>Paper</th>
<th>Reference</th>
<th>Github</th>
<th>Remarks</th>
<th>Annotation Type</th>
<th>Annotation Tool</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Lane detection</td>
<td>15th-Feb-2018</td>
<td>NA</td>
<td>Towards End-to-End Lane Detection: an Instance Segmentation Approach</td>
<td><a href="https://arxiv.org/abs/1802.05591">https://arxiv.org/abs/1802.05591</a></td>
<td><a href="https://github.com/MaybeShewill-CV/lanenet-lane-detection">https://github.com/MaybeShewill-CV/lanenet-lane-detection</a></td>
<td>Keras/Tensorflow</td>
<td>polylines for lane markings</td>
<td>NA</td>
</tr>
<tr>
<td>2</td>
<td>Lane detection</td>
<td></td>
<td>GCN</td>
<td>End to End Video Segmentation for Driving : Lane Detection For Autonomous Car</td>
<td>NA</td>
<td><a href="https://github.com/wenhuizhang/autoCar">https://github.com/wenhuizhang/autoCar</a></td>
<td>Raspberry Pi</td>
<td>NA</td>
<td>NA</td>
</tr>
<tr>
<td>3</td>
<td>Lane and Road Marking Detection and Recognition</td>
<td>17th-Oct-2017</td>
<td>VPGNet</td>
<td>VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition</td>
<td><a href="https://arxiv.org/abs/1710.06288">https://arxiv.org/abs/1710.06288</a></td>
<td><a href="https://github.com/SeokjuLee/VPGNet">https://github.com/SeokjuLee/VPGNet</a></td>
<td>Caffe</td>
<td>NA</td>
<td>NA</td>
</tr>
<tr>
<td>4</td>
<td>Lane detection</td>
<td>17th-Dec-2017</td>
<td>“spatial CNN(SCNN)”</td>
<td>Spatial As Deep: Spatial CNN for Traffic Scene Understanding</td>
<td><a href="https://arxiv.org/abs/1712.06080">https://arxiv.org/abs/1712.06080</a></td>
<td><a href="https://github.com/XingangPan/SCNN">https://github.com/XingangPan/SCNN</a></td>
<td>Caffe</td>
<td>NA</td>
<td>NA</td>
</tr>
<tr>
<td>5</td>
<td>Lane detection</td>
<td></td>
<td>SCNN+Lanenet</td>
<td>Agnostic Lane Detection</td>
<td>NA</td>
<td><a href="https://github.com/cardwing/Codes-for-Lane-Detection">https://github.com/cardwing/Codes-for-Lane-Detection</a></td>
<td>“TensorFlow+PyTorchCustom implementation”</td>
<td>NA</td>
<td>NA</td>
</tr>
<tr>
<td>6</td>
<td>Lane detection</td>
<td>1th-Feb-2019</td>
<td>NA</td>
<td>End-to-end Lane Detection through Differentiable Least-Squares Fitting</td>
<td><a href="https://arxiv.org/abs/1902.00293v1">https://arxiv.org/abs/1902.00293v1</a></td>
<td><a href="https://github.com/wvangansbeke/LaneDetection_End2End">https://github.com/wvangansbeke/LaneDetection_End2End</a></td>
<td>Pytorch</td>
<td>NA</td>
<td>NA</td>
</tr>
<tr>
<td>7</td>
<td>3D lane detection</td>
<td>27th-Nov_2018</td>
<td>NA</td>
<td>3D-LaneNet: end-to-end 3D multiple lane detection</td>
<td><a href="https://www.researchgate.net/publication/329206980_3D-LaneNet_end-to-end_3D_multiple_lane_detection">https://www.researchgate.net/publication/329206980_3D-LaneNet_end-to-end_3D_multiple_lane_detection</a></td>
<td>NA</td>
<td>From images</td>
<td>NA</td>
<td>NA</td>
</tr>
<tr>
<td>8</td>
<td></td>
<td></td>
<td>ReNet</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>9</td>
<td></td>
<td></td>
<td>MRFNet</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table><h4 id="*-road-profile-datasets--"><a name="*-road-profile-datasets--" href="#*-road-profile-datasets--"></a>* Road profile datasets -</h4><table>
<thead>
<tr>
<th>Slno</th>
<th>Name</th>
<th>Dataset reference</th>
<th>Paper</th>
<th>Dataset size</th>
<th>Total images</th>
<th>Annotation type</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>TuSimple</td>
<td><a href="https://github.com/TuSimple/tusimple-benchmark/blob/master/doc/lane_detection/readme.md">https://github.com/TuSimple/tusimple-benchmark/blob/master/doc/lane_detection/readme.md</a></td>
<td></td>
<td>10000 one-second-long video clips of 20 frames each</td>
<td>200000</td>
<td>Polylines</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>CULane</td>
<td><a href="https://xingangpan.github.io/projects/CULane.html">https://xingangpan.github.io/projects/CULane.html</a></td>
<td></td>
<td>88880 for training set 9675 for validation set 34680 for test set</td>
<td>133235</td>
<td></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Caltech lanes</td>
<td><a href="http://www.mohamedaly.info/datasets/caltech-lanes">http://www.mohamedaly.info/datasets/caltech-lanes</a></td>
<td></td>
<td>cordova1 with 250 frames cordova2 with 406 frames washington1 with 337 frames washington2 with 232 frames</td>
<td>1225</td>
<td></td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>BDD100K</td>
<td><a href="https://bdd-data.berkeley.edu/">https://bdd-data.berkeley.edu/</a></td>
<td><a href="https://arxiv.org/abs/1805.04687">https://arxiv.org/abs/1805.04687</a></td>
<td></td>
<td>100000</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>Carla Simulator</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>KITTI</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>CityPerson</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8</td>
<td>Cityscapes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>9</td>
<td>ApolloScape</td>
<td></td>
<td></td>
<td>Not available in existing open dataset May be in future work</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>Mapillary</td>
<td></td>
<td></td>
<td>18000 training 2000 validation 5000 test</td>
<td>25000</td>
<td></td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>Road Marking Dataset</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>12</td>
<td>KITTI Road</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>13</td>
<td>VPGNet</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table><p><img src="roadprofiledatasets.png" alt="roadprofiledatasets"></p><h4 id="*-more-info-on-tusimple-dataset--"><a name="*-more-info-on-tusimple-dataset--" href="#*-more-info-on-tusimple-dataset--"></a>* More info on TuSimple Dataset -</h4><ul>
<li><a href="https://github.com/TuSimple/tusimple-benchmark/tree/master/doc/lane_detection/assets/examples">https://github.com/TuSimple/tusimple-benchmark/tree/master/doc/lane_detection/assets/examples</a></li><li><a href="https://github.com/TuSimple/tusimple-benchmark/blob/master/example/lane_demo.ipynb">https://github.com/TuSimple/tusimple-benchmark/blob/master/example/lane_demo.ipynb</a></li><li><a href="https://github.com/TuSimple/tusimple-benchmark/blob/master/doc/lane_detection/readme.md">https://github.com/TuSimple/tusimple-benchmark/blob/master/doc/lane_detection/readme.md</a></li></ul><h4 id="*-2.-road-profile-dnn-hunt-and-experimentation-to-extract-road-profiles-(line-geometry-like-lanes,-road-edges-etc.)-which-has-the-geometric-sense-(x,y,z,y,p,r-w.r.t.-camera-position-as-0,0,0)"><a name="*-2.-road-profile-dnn-hunt-and-experimentation-to-extract-road-profiles-(line-geometry-like-lanes,-road-edges-etc.)-which-has-the-geometric-sense-(x,y,z,y,p,r-w.r.t.-camera-position-as-0,0,0)" href="#*-2.-road-profile-dnn-hunt-and-experimentation-to-extract-road-profiles-(line-geometry-like-lanes,-road-edges-etc.)-which-has-the-geometric-sense-(x,y,z,y,p,r-w.r.t.-camera-position-as-0,0,0)"></a>* 2. Road profile DNN hunt and experimentation to extract road profiles (line geometry like lanes, road edges etc.) which has the geometric sense (x,y,z,y,p,r w.r.t. camera position as 0,0,0)</h4><ul>
<li>as measured by:<ul>
<li>Total number of DNNArch referred - not less than 5</li><li>Total number of DNNArch deployed locally and experimented with ob gaze dataset - not less than 2</li><li>Clarity and depth of documentation on the learning of the research - excel sheet, docs, workflow diagrams in internal git-rep<br>  a) Documentation, experiment results on the understanding and providing insights on differences in the architectural differences this class of DNNArch w.r.t. object detection/segmentation DNNArch (in particular mask_rcnn). For example, reasoning on:<pre><code data-origin="<pre><code>  i) Why lanet is better than mask_rcnn for lane segmentation?
  ii) Can lanet be used in place of mask_rcnn for object segmentation for object segmentation?
</code></pre>">  i) Why lanet is better than mask_rcnn for lane segmentation?
  ii) Can lanet be used in place of mask_rcnn for object segmentation for object segmentation?
</code></pre></li></ul>
</li></ul><h4 id="*-scnn-notes"><a name="*-scnn-notes" href="#*-scnn-notes"></a>* SCNN Notes</h4><ul>
<li>CNNs are great for extracting semantics from raw pixels but perform poorly on capturing the spatial relationships (e.g. rotational and translational relationships) of pixels in a frame. These spatial relationships, however, are important for the task of lane detection, where there are strong shape priors but weak appearance coherences.</li><li>For example, it is hard to determine traffic poles solely by extracting semantic features as they lack distinct and coherent appearance cues and are often occluded.</li><li>Spatial CNN (SCNN) proposes an architecture which “generalizes traditional deep layer-by-layer convolutions to slice-by slice convolutions within feature maps”.</li><li>In a traditional layer-by-layer CNN, each convolution layer receives input from its preceding layer, applies convolutions and nonlinear activation, and sends the output to the succeeding layer.</li><li>SCNN takes this a step further by treating individual feature map rows and columns as the “layers”, applying the same process sequentially (where sequentially means that a slice passes information to the succeeding slice only after it has received information from the preceding slices), allowing message passing of pixel information between neurons within the same layer, effectively increasing emphasis on spatial information.</li></ul><h4 id="*-diff-b/w-cnn,-scnn-and-lanenet"><a name="*-diff-b/w-cnn,-scnn-and-lanenet" href="#*-diff-b/w-cnn,-scnn-and-lanenet"></a>* Diff b/w cnn, scnn and lanenet</h4><table>
<thead>
<tr>
<th>CNN</th>
<th>SCNN</th>
<th>Lanenet</th>
</tr>
</thead>
<tbody>
<tr>
<td>CNNs are great for extracting semantics from raw pixels but perform poorly on capturing the spatial relationships (e.g. rotational and translational relationships) of pixels in a frame. These spatial relationships, however, are important for the task of lane detection, where there are strong shape priors but weak appearance coherences.</td>
<td>Spatial CNN (SCNN) proposes an architecture which “generalizes traditional deep layer-by-layer convolutions to slice-by slice convolutions within feature maps”.</td>
<td>Lanenet outputs pixel per frame. To fit a curve through these pixels to get the lane parametrization. The lane pixels are first projected into a ”bird’s-eye view” representation, using a fixed transformation matrix</td>
</tr>
<tr>
<td>For example, it is hard to determine traffic poles solely by extracting semantic features as they lack distinct and coherent appearance cues and are often occluded.</td>
<td>SCNN takes this a step further by treating individual feature map rows and columns as the “layers”, applying the same process sequentially (where sequentially means that a slice passes information to the succeeding slice only after it has received information from the preceding slices), allowing message passing of pixel information between neurons within the same layer, effectively increasing emphasis on spatial information.</td>
<td>Due to the fact that the transformation parameters are fixed for all images, this raises issues when non flat ground-planes are encountered, e.g. in slopes. To alleviate this problem, we train a network, referred to as H-Net, that estimates the parameters of an ”ideal” perspective transformation, conditioned on the input image.</td>
</tr>
<tr>
<td></td>
<td>Gets curve line from probability map from matlab</td>
<td>Most popular detect-and-segment approaches (e.g. [14] Mask R-CNN, [38] J. Dai, K. He, and J. Sun. Instance-aware semantic segmentation via multi-task network cascades. In CVPR, 2016) are not ideal for lane instance segmentation, since bounding box detection is more suited for compact objects, which lanes are not.</td>
</tr>
</tbody>
</table><h4 id="*-h-net-notes"><a name="*-h-net-notes" href="#*-h-net-notes"></a>* H-Net Notes</h4><ul>
<li>The output of LaneNet is a collection of pixels per lane</li><li>Fitting a polynomial through these pixels in the original image space is not ideal, as one has to resort to higher order polynomials to be able to cope with curved lanes.</li><li>A frequently used solution to this problem is to project the image into a ”bird’s-eye view” representation, in which lanes are parallel to each other and as such, curved lanes can be fitted with a 2nd to 3rd order polynomial.</li><li>However, in these cases the transformation matrix H is calculated once, and kept fixed for all images. Typically, this leads to errors under ground-plane changes where the vanishing-point, which is projected onto infinity, shifts up or downwards.</li><li>To resolve this issue we train a neural network, H-Net, with a custom loss function: the network is optimized end-toend to predict the parameters of a perspective transformation H, in which the transformed lane points can be optimally fitted with a 2nd or 3rd order polynomial. The prediction is conditioned on the input image, allowing the network to adapt the projection parameters under ground-plane changes, so that the lane fitting will still be correct. </li><li>In our case, <strong>H has 6 degrees of freedom</strong>:<pre><code data-origin="<pre><code>H =  a b c
      0 d e
      0 f 1 ]
</code></pre>">H =  a b c
      0 d e
      0 f 1 ]
</code></pre>The zeros are placed to enforce the constraint that horizontal lines remain horizontal under the transformation.</li></ul><h4 id="lane-datasets-for-lane-detection--"><a name="lane-datasets-for-lane-detection--" href="#lane-datasets-for-lane-detection--"></a>Lane Datasets for Lane Detection -</h4><p><strong>l. CULane</strong> </p><ul>
<li>Culane is a lane dataset for academic research related to traffic lane detection. </li><li>This dataset contains the data of different drivers of <strong>Beijing</strong>. </li><li>There are <strong>6 different cameras</strong> are mounted on the vehicles. </li><li>They have collected more than <strong>55 hours</strong> of videos which have <strong>133,235 frames</strong>. </li><li>The dataset is divided as training set and validation set. </li><li><strong>Training</strong> set has <strong>88880 frames</strong> and the <strong>validation</strong> set or test set has <strong>9675</strong> frames. </li><li>Further the test set is divided into normal and <strong>8 challenging</strong> categories. </li><li>For every frame traffic lanes are interpreted with <strong>cubic splines</strong>. </li><li>If the lanes are occluded by the vehicles or not visible that lanes are also interpreted as per context</li><li>Types of scene in CUlane dataset</li></ul><table>
<thead>
<tr>
<th>Type of scene</th>
<th>Percentage of data in dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td>Normal</td>
<td>27.7%</td>
</tr>
<tr>
<td>Crowded</td>
<td>23.4%</td>
</tr>
<tr>
<td>Night</td>
<td>20.3%</td>
</tr>
<tr>
<td>No line</td>
<td>11.7%</td>
</tr>
<tr>
<td>Shadow</td>
<td>2.7%</td>
</tr>
<tr>
<td>Arrow</td>
<td>2.6%</td>
</tr>
<tr>
<td>Dazzle light</td>
<td>1.4%</td>
</tr>
<tr>
<td>Curve</td>
<td>1.2%</td>
</tr>
<tr>
<td>Crossroad</td>
<td>9.0%</td>
</tr>
</tbody>
</table><p><strong>ll. Caltech</strong></p><ul>
<li>The Caltech Lanes dataset has four clips</li><li>The clips are taken on the roads of Pasadena, California. </li><li>The clips are taken at different day times. </li><li>There are total 1225 individual frames which are taken from the camera mounted on the Alice.</li><li>Dataset contains the labeled lanes.</li><li>The dataset is divided into 4 clips as shown below</li></ul><table>
<thead>
<tr>
<th>Name of the Clip</th>
<th>Frames in the Clip</th>
</tr>
</thead>
<tbody>
<tr>
<td>cordova1</td>
<td>250</td>
</tr>
<tr>
<td>cordova2</td>
<td>406</td>
</tr>
<tr>
<td>washington1</td>
<td>337</td>
</tr>
<tr>
<td>washington2</td>
<td>232</td>
</tr>
</tbody>
</table><ul>
<li>Clips are gathered from various types of urban streets.</li><li>Dataset contains the streets with shadows and without shadows and straight and curved paths. Straight and curved streets.</li><li>The 1224 visible lanes are hand-labeled in the all 4 clips.</li><li>There are total 4172 marked lanes.</li><li>Clip cordoval1 there are 919 marked lanes, curvatures and writing on the few roads.</li><li>Clip cordova2 has 1048 marked lanes, various pavements and the sun facing the vehicle.</li><li>Clip Washington1 has 1274 marked lanes, lots of shadows on the road and passing vehicles.</li><li>Clip Washington 2 has 931 marked lanes, passing vehicles and street writing.</li></ul><p><strong>lll. NEXET </strong></p><ul>
<li>NEXET is the largest and diverse automotive street dataset released by Nexar.</li><li>The data in the dataset is collect by using the Nexar app.</li><li>From total 77 countries the data is collected.</li><li>There are total 50,000 labeled and tagged frames in training dataset and 41190 images in testing dataset.</li><li>Data is collected for various day and night time.</li><li>The dataset aggregate the images and videos taken at different weather conditions, lighting conditions, and geographical locations.</li><li>The researchers have labeled the rear<br>part of nearby cars which could become caught up in a crash.</li></ul><p><strong>lV. DIML </strong></p><ul>
<li>The images are captured using the OV10630 image sensor.</li><li>There are 470 video sequences in dataset and the image size is 1280 × 800.</li><li>The roads the dataset covered are highway, tunnel, route and urban.</li><li>The scenes are captured in day, night, sunrise and sunset time.</li><li>The weather conditions covered in dataset are rainy, cloudy and clear.</li><li>Additionally, the dataset also covers the conditions like car lamps, lens flare, white lamp, street lamp, yellow lamp.</li><li>The dataset also includes different scenarios like pedestrians, traffic jam and obstacles.</li><li>Categories of DIML dataset<table>
<thead>
<tr>
<th>Category</th>
<th>Scenario covered</th>
<th>Conditions covered</th>
</tr>
</thead>
<tbody>
<tr>
<td>DIML-LD-1</td>
<td>highway scenario</td>
<td>1. Capture time- Sunset, sunrise, day, night<br>2. Weather conditions-cloudy, rainy, clear.<br>3. Other conditions-car lamps, street lamps, lens flare.</td>
</tr>
<tr>
<td>DIML-LD-2</td>
<td>Route scenario</td>
<td>1. capture time- Sunset, sunrise, day, night<br>2. Weather conditions cloudy, rainy, clear.<br>3. Other conditions-car lamps, street lamps, lens flare.</td>
</tr>
<tr>
<td>DIML-LD-3</td>
<td>Urban road</td>
<td>1. capture time- Sunset, sunrise, day, night<br>2. Weather conditions cloudy, rainy, clear.<br>3. Other conditions-car lamps, street lamps, lens flare.</td>
</tr>
<tr>
<td>DIML-LD-4</td>
<td>Tunnel</td>
<td>Other conditions- yellow lamp, white lamp.</td>
</tr>
</tbody>
</table>
</li></ul><p><strong>V. KITTI Vision Benchmark Suite</strong></p><ul>
<li>The KITTI dataset uses two 2 high resolution grayscale andcolor video cameras.</li><li>For accurate ground truth a Velodyne laser scanner and a GPS localization system is used.</li><li>The dataset collected the data by driving on the mid city of Karlsruhe and, urban road and rural roads.</li><li>There are up to 30 pedestrians and 15 cars are visible per image.</li><li>The recording platform of the lane dataset is Volkswagen Passat B6.</li><li>The actuators are for the pedals and the steering wheels.</li><li>The data is recorded using an eight core i7 computer set with a RAID system, running Ubuntu Linux and a real-time database.</li><li>The captured camera images are cropped to size 1382 x 512 pixels.</li></ul><p><strong>Vl. TuSimple Benchmark</strong></p><ul>
<li>The Tusimple dataset has divided the road into 2 main groups called static objects and dynamic objects.</li><li>On the highway the lane marking is the main static component. Lane markings guide the vehicle to drive on the highway interactively and safely.</li><li>The dataset contains 7000 video clips of one second. </li><li>Each video clip contains 20 frames.</li><li>The last frame of each video is labeled.</li><li>Polyline annotation is used for lane marking.</li><li>The data is collected in medium and good weather condition and at different day time.</li><li>It covers 2, 3 and 4 lanes on the highway as well as various traffic conditions.</li><li>The training set contains 3626 videos and 3626 annotated frames. The testing set<br>includes 2782 video clips</li></ul><p><strong>Vll. The UAH Driveset</strong></p><ul>
<li>The UAH driverset dataset is captured by using the DriveSafe a driving monitoring app. <em> </em> Data is recoded in different environment. The application was run on 3 different behaviors (aggressive, normal and drowsy) and six various drivers and vehicles on 2 types of streets (secondary road and motor way).</li><li>The dataset contains 500 minutes driving raw data plus some semantic information.</li><li>The drive safe app monitors and alerts you while driving.</li><li>It also detects the roughness in your vehicle’s motion, processes the location in lane and the lane changes, the maximum permitted speed and number of lanes of the street , and detects the in front vehicles and the distance/time kept to them and many more things.</li></ul><p><strong>Vlll. BDD100K</strong></p><ul>
<li>BDD100K is the largest and diverse dataset released by TLDR.</li><li>This dataset is rich with annotations. With the help of Nexar the BDD100K database is released.</li><li>The BDD100K dataset contains 100,000. Each video is of 40 second long with 30 frames per second. </li><li>Videos are captured from the diverse locations in US.</li><li>The videos come with GPS/IMU information recorded by cell-phones to demonstrate uneven driving trajectories.</li><li>Dataset covers the different weather conditions like rainy, sunny, overcast and different times of day such as daytime and nighttime.</li><li>A keyframe at the tenth second from every video is sampled and annotation is given to that keyframes.</li><li>These are labeled at various levels like road object bounding boxes, image tagging, lane markings, full-frame instance segmentation and drivable areas.</li></ul><p><strong>COMPARISON AMONG DIFFERENT LANE DATASET - </strong></p><table>
<thead>
<tr>
<th></th>
<th>CULane</th>
<th>Caltech</th>
<th>NEXET</th>
<th>DIML</th>
<th>KITTI</th>
<th>TuSimple</th>
<th>UAH</th>
<th>BDD100K</th>
<th>Cityscape</th>
<th>Apolloscape</th>
<th>Mapillary Vistas</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sequences</td>
<td>More than 55 hrs of videos</td>
<td>4 clips</td>
<td>NA</td>
<td>470</td>
<td>22</td>
<td>700</td>
<td>500 min video</td>
<td>100000</td>
<td>7 months</td>
<td>4</td>
<td>NA</td>
</tr>
<tr>
<td>Images</td>
<td>133235</td>
<td>1225</td>
<td>50000</td>
<td>-</td>
<td>14999</td>
<td>140000</td>
<td>NA</td>
<td>120000000</td>
<td>5k fine annotated and 20k weakly annotated images</td>
<td>147k</td>
<td>25k</td>
</tr>
<tr>
<td>Classes</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>34</td>
<td></td>
<td></td>
<td>19</td>
<td>34</td>
<td>36</td>
<td>66</td>
</tr>
<tr>
<td>Multiple citis</td>
<td>NO</td>
<td>NO</td>
<td>Yes 77 cities</td>
<td>YES</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>Yes 50 cities</td>
<td>NO</td>
<td>YES</td>
</tr>
<tr>
<td>Multiple weathers</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>YES</td>
</tr>
<tr>
<td>Multiple times of day</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>YES</td>
</tr>
<tr>
<td>Multiple scene type</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>YES</td>
</tr>
<tr>
<td>Multiple cameras</td>
<td>YES</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
</tr>
<tr>
<td>Multiple street type</td>
<td>NO</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
</tr>
<tr>
<td>Streets labeled</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>NO</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
<td>YES</td>
</tr>
</tbody>
</table>

<footer style="position:fixed; font-size:.8em; text-align:right; bottom:0px; margin-left:-25px; height:20px; width:100%;">generated by <a href="http://pad.haroopress.com" target="_blank">haroopad</a></footer>
</body>
</html>
